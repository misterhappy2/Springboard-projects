{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Defaults\n",
    "\n",
    "## Springboard, spring 2018 Capstone Project #1\n",
    "## Brent Jensen\n",
    "\n",
    "### Overview: \n",
    "The health of the credit card industry is best measured not by the number of customers, but rather by the number who pay their bills. High rates of defaults can cut into profits, or worse, can ruin a bank.\n",
    "Banks want to keep accounts current. They seek to minimize risk of any kind, but most of all defaults on loans.\n",
    "Delinquencies were low in 2017 compared to history, but will very likely rise again if the economy worsens.\n",
    "#### The Client: \n",
    "If a bank could predict which clients were likely to default, they might avoid risk by: not extending additional credit, or by contacting at-risk clients to help or intervene.\n",
    "But how can a bank know which clients will default? Are there any clues?\n",
    "We can examine data and find correlations among clients, and predict likely clients who might default.\n",
    "#### The Data: \n",
    "UC Irvine Machine Learning Repository has a file on credit card defaults from Taiwan in 2005. \n",
    "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients The set of 30,000 includes whether the account defaulted in the next month, as well as demographic and account data, including: gender, marital status, education attained, age, amount of household credit, and 6 month history of credit balance, payments, and defaults.\n",
    "\n",
    "### CreditCardEDA \n",
    "a deep dive into the information.  histograms and statistics to get a visual idea of the data.  From the start, I knew I would wrangle the data and apply machine learning to predict default.  But along the way, there are insights to be found.  \n",
    "For instance, I observed that credit amounts were rising fast.  I learned that payment history correlates most strongly with default, but other variables have slight correlation.  (Thus, I knew I could continue with my thesis that machine learning would give good results.)  \n",
    "\n",
    "### Data Cleaning \n",
    "The data had gaps, outliers, and numerical data that could be changed to categorical.  This module contains all the preprocessing needed before machine learning.\n",
    "\n",
    "### FeatureEngineering \n",
    "All the machine learning, and most of the tuning and feature engineering attempts I made along the way to achieve my results\n",
    "\n",
    "### StatisticsCap1 \n",
    "a different Springboard project involving statistic analysis.  It involves the same data set, so I put it into the same repository.  \n",
    "\n",
    "### Story telling Springboard project \n",
    "another Springboard project where I tell a story.  Unrelated to my predictor, but it also uses the same data.\n",
    "\n",
    "### Lower Risk, Fewer Losses\n",
    "slides to my report\n",
    "https://drive.google.com/file/d/1AwR7tqJAg_BDH1GJwEk0j2Rfz6x8VwCu/view?usp=sharing\n",
    "\n",
    "### Loser Risk, Fewer Losses\n",
    "my final report\n",
    "https://drive.google.com/file/d/1R30fbRS_xLz42vgBwdMPjhDwYAY-9sfM/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
